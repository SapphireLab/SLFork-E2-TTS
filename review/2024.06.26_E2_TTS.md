# E2 TTS

<details>
<summary>基本信息</summary>

- 标题: E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS
- 作者:
  - 01 Sefik Emre Eskimez
  - 02 Xiaofei Wang (王晓飞)
  - 03 Manthan Thakker
  - 04 Canrun Li
  - 05 Chung-Hsien Tsai
  - 06 Zhen Xiao
  - 07 Hemin Yang
  - 08 Zirun Zhu
  - 09 Min Tang 
  - 10 Xu TanXu_Tan_(谭旭)
  - 11 Yanqing Liu
  - 12 Sheng Zhao (赵胜)
  - 13 Naoyuki Kanda
- 机构:
  - Microsoft
- 时间:
  - 预印时间: 2024.06.26 ArXiv v1
  - 更新笔记: 2024.07.22
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.18009)
  <!-- - [DOI]() -->
  <!-- - [Github]() -->
  - [Demo](https://aka.ms/e2tts/)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - 非自回归 NAR
  - 零样本 Zero-Shot
  - 流匹配 Flow-Matching
- 页数: 8
- 引用: 38
- 被引: [1](https://scholar.google.com/scholar?cites=3651905598761387814&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII) by **MELLE**
- 数据:
  - 2023.09.15 Libriheavy
- 对比:
  - 2023.01.05 VALL-E
  - 2023.06.23 Voicebox
  - 2024.03.05 NaturalSpeech3
- 复现:
  - 2024.07.10 [lucidrains/e2-tts-pytorch](https://github.com/lucidrains/e2-tts-pytorch)

</details>

## Abstract: 摘要

> This paper introduces ***Embarrassingly Easy Text-to-Speech (E2 TTS)***, a fully non-autoregressive zero-shot text-to-speech system that offers human-level naturalness and state-of-the-art speaker similarity and intelligibility.
> In the ***E2 TTS*** framework, the text input is converted into a character sequence with filler tokens.
> The flow-matching-based mel spectrogram generator is then trained based on the audio infilling task.
> Unlike many previous works, it does not require additional components (e.g., duration model, grapheme-to-phoneme) or complex techniques (e.g., monotonic alignment search).
> Despite its simplicity, ***E2 TTS*** achieves state-of-the-art zero-shot TTS capabilities that are comparable to or surpass previous works, including Voicebox and NaturalSpeech 3.
> The simplicity of ***E2 TTS*** also allows for flexibility in the input representation.
> We propose several variants of ***E2 TTS*** to improve usability during inference.
> See this https [URL](https://aka.ms/e2tts/) for demo samples.

## 1.Introduction: 引言

> In recent years, text-to-speech (TTS) systems have seen significant improvements \cite{ren2019fastspeech,ren2020fastspeech,kim2020glow,kong2020hifi}, achieving a level of naturalness that is indistinguishable from human speech \cite{tan2024naturalspeech}. This advancement has further led to research efforts to generate natural speech for any speaker from a short audio sample, often referred to as an audio prompt. Early studies of zero-shot TTS used speaker embedding to condition the TTS system \cite{arik2018neural,jia2018transfer}. More recently, VALL-E \cite{wang2023neural} proposed formulating the zero-shot TTS problem as a language modeling problem in the neural codec domain, achieving significantly improved speaker similarity while maintaining a simplistic model architecture. Various extensions were proposed to improve stability \cite{wang2023speechx,du2024vall,xin2024rall}, and VALL-E 2 \cite{chen2024valle} recently achieved human-level zero-shot TTS with techniques including repetition-aware sampling and grouped code modeling.
>
> While the neural codec language model-based zero-shot TTS achieved promising results, there are still a few limitations based on its auto-regressive (AR) model-based architecture. Firstly, because the codec token needs to be sampled sequentially, it inevitably increases the inference latency. Secondly, a dedicated effort to figure out the best tokenizer (both for text tokens and audio tokens) is necessary to achieve the best quality \cite{wang2023speechx}. Thirdly, it is required to use some tricks to stably handle long sequences of audio codecs, such as the combination of AR and non-autoregressive (NAR) modeling \cite{wang2023neural,chen2024valle}, multi-scale transformer \cite{yang2023uniaudio}, grouped code modeling \cite{chen2024valle}.
>
> Meanwhile, several fully NAR zero-shot TTS models have been proposed with promising results. Unlike AR-based models, fully NAR models enjoy fast inference based on parallel processing. NaturalSpeech 2 \cite{shen2023naturalspeech} and NaturalSpeech 3 \cite{ju2024naturalspeech} estimate the latent vectors of a neural audio codec based on diffusion models \cite{ho2020denoising,song2020score}. Voicebox \cite{le2024voicebox} and MatchaTTS \cite{mehta2024matcha} used a flow-matching model \cite{lipman2022flow} conditioned by an input text. However, one notable challenge for such NAR models is how to obtain the alignment between the input text and the output audio, whose length is significantly different. NaturalSpeech 2, NaturalSpeech 3, and Voicebox used a frame-wise phoneme alignment for training the model. MatchaTTS, on the other hand, used monotonic alignment search (MAS) \cite{popov2021grad,kim2020glow} to automatically find the alignment between input and output. While MAS could alleviate the necessity of the frame-wise phoneme aligner, it still requires an independent duration model to estimate the duration of each phoneme during inference. More recently, E3 TTS \cite{gao2023e3} proposed using cross-attention from the input sequence, which required a carefully designed U-Net architecture \cite{ronneberger2015u}. 
> As such, fully NAR zero-shot TTS models require either an explicit duration model or a carefully designed architecture. One of our findings in this paper is that such techniques are not necessary to achieve high-quality zero-shot TTS, and they are sometimes even harmful to naturalness.\footnote{Concurrent with our work, Seed-TTS \cite{anastassiou2024seed} proposed a diffusion model-based zero-shot TTS, named Seed-TTS$_{DiT}$. Although it appears to share many similarities with our approach, the authors did not elaborate on the details of their model, making it challenging to compare with our work.}
>
> Another complexity in TTS systems is the choice of the text tokenizer. As discussed above, the AR-model-based system requires a careful selection of tokenizer to achieve the best result. On the other hand, most fully NAR models assume a monotonic alignment between text and output, with the exception of E3 TTS, which uses cross-attention. These models impose constraints on the input format and often require a text normalizer to avoid invalid input formats. When the model is trained based on phonemes, a grapheme-to-phoneme converter is additionally required.
> 
> In this paper, we propose ***Embarrassingly Easy TTS (E2 TTS)***, a fully NAR zero-shot TTS system with a surprisingly simple architecture. E2 TTS consists of only two modules: a flow-matching-based mel spectrogram generator and a vocoder.
> The text input is converted into a character sequence with filler tokens to match the length of the input character sequence and the output mel-filterbank sequence. The mel spectrogram generator, composed of a vanilla Transformer with U-Net style skip connections, is trained using a speech-infilling task \cite{le2024voicebox}.
> Despite its simplicity, E2 TTS achieves state-of-the-art zero-shot TTS capabilities that are comparable to, or surpass, previous works, including Voicebox and NaturalSpeech 3.
> The simplicity of E2 TTS also allows for flexibility in the input representation. We propose several variants of E2 TTS to improve usability during inference.

注: 对原文内容进行了重排
- 第三章 方法 对应 原文第二章
- 第四章 实验 对应 原文第三章前三节
- 第五章 结果 对应 原文第三章后三节
- 第六章 结论 对应 原文第四章

## 2.Related Works: 相关工作

~~None~~

## 3.Methodology: 方法

### 3.1.Training: 训练

### 3.2.Inference: 推理

### 3.3.Flow-Matching-Based Mel Spectrogram Generator: 基于流匹配的梅尔频谱生成器

### 3.4.Relationship to Voicebox: 与 Voicebox 的关联

### 3.5.Extension of E2 TTS: E2 TTS 的扩展

## 4.Experiments: 实验

### 4.1.Training Data: 训练数据

### 4.2.Model Configurations: 模型配置

### 4.3.Evaluation Data & Metrics: 评估数据及指标

## 5.Results: 结果

### 5.1.Main Results: 主要结果

### 5.2.Evaluation of E2 TTS Extensions: E2 TTS 扩展的评估

### 5.3.Analysis of the System Behavior: 系统行为分析

## 6.Conclusions: 结论

> We introduced ***E2 TTS***, a novel fully NAR zero-shot TTS. 
> In the ***E2 TTS*** framework, the text input is converted into a character sequence with filler tokens to match the length of the input character sequence and the output mel-filterbank sequence. 
> The flow-matching-based mel spectrogram generator is then trained based on the audio infilling task. 
> Despite its simplicity, ***E2 TTS*** achieved state-of-the-art zero-shot TTS capabilities that were comparable to or surpass previous works, including Voicebox and NaturalSpeech 3. 
> The simplicity of ***E2 TTS*** also allowed for flexibility in the input representation.
> We proposed several variants of ***E2 TTS*** to improve usability during inference.